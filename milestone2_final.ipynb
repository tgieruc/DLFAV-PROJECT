{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgieruc/miniconda3/envs/dlfav/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.insert(0, os.path.join(os.path.abspath(''), \"YOLOX\"))\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "import time\n",
    "\n",
    "from tracker import Tracker\n",
    "from pose_detector import PoseDetector\n",
    "from sequence_detector import SequenceDetector\n",
    "from pose_tracker import PoseTracker\n",
    "from visualizer import Visualizer\n",
    "from project_utils import identify_bbox\n",
    "\n",
    "environment = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if environment == \"colab\":\n",
    "    from IPython.display import display, Javascript, Image, clear_output\n",
    "    from base64 import b64decode, b64encode\n",
    "    import cv2, PIL, io\n",
    "     # Import PyDrive and associated libraries.\n",
    "    # This only needs to be done once per notebook.\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    # This only needs to be done once per notebook.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "\n",
    "    !gdown --id 1iYxlkO_RZJ_6iL6wdIbfjl8VprLlFp1c\n",
    "\n",
    "         # function to convert the JavaScript object into an OpenCV image\n",
    "    def js_to_image(js_reply):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "                js_reply: JavaScript object containing image from webcam\n",
    "        Returns:\n",
    "                img: OpenCV BGR image\n",
    "        \"\"\"\n",
    "        # decode base64 image\n",
    "        image_bytes = b64decode(js_reply.split(',')[1])\n",
    "        # convert bytes to numpy array\n",
    "        jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "        # decode numpy array into OpenCV BGR image\n",
    "        img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "        return img\n",
    "\n",
    "    # function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "    def bbox_to_bytes(bbox_array):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "                bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "        Returns:\n",
    "              bytes: Base64 image byte string\n",
    "        \"\"\"\n",
    "        # convert array into PIL image\n",
    "        bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
    "        iobuf = io.BytesIO()\n",
    "        # format bbox into png for return\n",
    "        bbox_PIL.save(iobuf, format='png')\n",
    "        # format return string\n",
    "        bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "        return bbox_bytes\n",
    "else:\n",
    "    cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pose_tracker import PoseTracker\n",
    "tracker = Tracker('person')\n",
    "pose_detector = PoseDetector(n_feature=8, n_output=3, checkpoint_path=\"pose_classification_simplified.ckpt\", simple_mode=True, model_based=False)\n",
    "sequence_detector = SequenceDetector([1,2], 10)\n",
    "pose_tracker = PoseTracker(pose_detector, sequence_detector, tracker)\n",
    "visualizer = Visualizer(environment)\n",
    "poi = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pip-install-4y87dran/openpifpaf_6aac84dbeb8e466293e96168b9f10b8d/src/openpifpaf/csrc/src/cif_hr.cpp:102: UserInfo: resizing cifhr buffer\n",
      "/tmp/pip-install-4y87dran/openpifpaf_6aac84dbeb8e466293e96168b9f10b8d/src/openpifpaf/csrc/src/occupancy.cpp:53: UserInfo: resizing occupancy buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgieruc/DLFAV-PROJECT/pose_detector.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  _, pred = torch.max(F.softmax(self.keypoint_to_pose_net(torch.from_numpy(keypoints.astype(np.float32)).cuda(device))).data, dim=1)\n",
      "/home/tgieruc/miniconda3/envs/dlfav/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     42\u001B[0m     check_poi \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 43\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43mpose_tracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]) :\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m outputs[i,\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m==\u001B[39mpoi:\n",
      "File \u001B[0;32m~/DLFAV-PROJECT/tracker.py:27\u001B[0m, in \u001B[0;36mTracker.update\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[0;32m---> 27\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisual\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbox_nums\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/DLFAV-PROJECT/detector.py:50\u001B[0m, in \u001B[0;36mDetector.detect\u001B[0;34m(self, raw_img, visual, conf)\u001B[0m\n\u001B[1;32m     47\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 50\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m outputs \u001B[38;5;241m=\u001B[39m postprocess(\n\u001B[1;32m     53\u001B[0m     outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexp\u001B[38;5;241m.\u001B[39mnum_classes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexp\u001B[38;5;241m.\u001B[39mtest_conf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexp\u001B[38;5;241m.\u001B[39mnmsthre  \u001B[38;5;66;03m# TODO:用户可更改\u001B[39;00m\n\u001B[1;32m     54\u001B[0m )[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m outputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/dlfav/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/DLFAV-PROJECT/YOLOX/yolox/models/yolox.py:46\u001B[0m, in \u001B[0;36mYOLOX.forward\u001B[0;34m(self, x, targets)\u001B[0m\n\u001B[1;32m     37\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss,\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miou_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: iou_loss,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_fg\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_fg,\n\u001B[1;32m     44\u001B[0m     }\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 46\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpn_outs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/dlfav/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/DLFAV-PROJECT/YOLOX/yolox/models/yolo_head.py:214\u001B[0m, in \u001B[0;36mYOLOXHead.forward\u001B[0;34m(self, xin, labels, imgs)\u001B[0m\n\u001B[1;32m    210\u001B[0m outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(\n\u001B[1;32m    211\u001B[0m     [x\u001B[38;5;241m.\u001B[39mflatten(start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m outputs], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    212\u001B[0m )\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_in_inference:\n\u001B[0;32m--> 214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxin\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/DLFAV-PROJECT/YOLOX/yolox/models/yolo_head.py:248\u001B[0m, in \u001B[0;36mYOLOXHead.decode_outputs\u001B[0;34m(self, outputs, dtype)\u001B[0m\n\u001B[1;32m    245\u001B[0m     shape \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    246\u001B[0m     strides\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mfull((\u001B[38;5;241m*\u001B[39mshape, \u001B[38;5;241m1\u001B[39m), stride))\n\u001B[0;32m--> 248\u001B[0m grids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m strides \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(strides, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mtype(dtype)\n\u001B[1;32m    251\u001B[0m outputs[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, :\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m (outputs[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, :\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m+\u001B[39m grids) \u001B[38;5;241m*\u001B[39m strides\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if environment == \"colab\":\n",
    "    # start streaming video from webcam\n",
    "    video_stream()\n",
    "    # label for video\n",
    "    label_html = 'Capturing...'\n",
    "    # initialze bounding box to empty\n",
    "    bbox = ''\n",
    "    #count = 0\n",
    "    \n",
    "    \n",
    "state = 0\n",
    "poi = None\n",
    "\n",
    "while True:\n",
    "    if environment == \"colab\":\n",
    "        js_reply = video_frame(label_html, bbox)\n",
    "        if not js_reply:\n",
    "            break\n",
    "        # convert JS response to OpenCV Image\n",
    "        img = js_to_image(js_reply[\"img\"])\n",
    "        # create transparent overlay for bounding box\n",
    "        bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
    "    else:\n",
    "        _, img = cap.read()\n",
    "        if img is None:\n",
    "            break\n",
    "            \n",
    "    im = imutils.resize(img, height=500)\n",
    "\n",
    "    #initialization\n",
    "    if state == 0:\n",
    "        outputs, poi = pose_tracker.update(im)\n",
    "        pose_in_progress = False\n",
    "        for i,  p in pose_tracker.sequence_detector.people.items():\n",
    "            if (p.i_seq > 0) and (i != poi):\n",
    "                pose_in_progress = True\n",
    "                break\n",
    "        if (poi != None) and (not pose_in_progress):\n",
    "            last_detected = time.time()\n",
    "            state = 1\n",
    "    if state == 1:\n",
    "        check_poi = 0\n",
    "        outputs = np.array(pose_tracker.tracker.update(im))\n",
    "        for i in range(outputs.shape[0]) :\n",
    "            if outputs[i,4] ==poi:\n",
    "                check_poi = 1\n",
    "        if (check_poi == 0) or (time.time()-last_detected >1):\n",
    "            state = 0\n",
    "        \n",
    "    visualizer.show(im, outputs, poi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}